<!-- 变更日志
v2 2026-02-18: 根据四角色评审修订——增加自注意力数值例题、补充拓展阅读文献、增加向量数据库选型建议
v1 2026-02-18: 初稿
-->

# 第二章 认知智能理论基础

---

## 学习目标

完成本章后，读者应能够：

1. 描述认知科学中"感知—记忆—推理—语言"四大认知功能，并将其映射到水利认知智能系统的功能模块；
2. 掌握知识表示的主要形式化方法（逻辑表示、语义网络、框架、本体），并比较其在水利领域的适用性；
3. 理解自然语言处理（NLP）从规则方法到预训练语言模型的技术演进脉络；
4. 阐述Transformer架构的核心机制（自注意力、位置编码、多头注意力），并说明其对水利文本处理的意义；
5. 说明大语言模型（LLM）的预训练-微调范式、涌现能力与局限性。

> **章首衔接（承接ch01）**
> 上一章界定了水利认知智能的问题域、学科坐标和能力边界。本章从理论层面回答"认知智能建立在哪些学科基础之上"，为后续章节的知识图谱构建（ch03）、大语言模型领域适配（ch04）和RAG系统设计（ch05）提供必要的理论储备。

---

## 2.1 认知科学视角：什么是"认知"

### 2.1.1 认知科学的基本框架

认知科学（Cognitive Science）是研究心智与智能本质的跨学科领域，融合了心理学、语言学、神经科学、哲学、人工智能和人类学（Gardner, 1985）。对于水利认知智能而言，认知科学提供了一个理解"智能系统应当具备什么能力"的参照框架。

认知科学将人类认知过程分解为若干基本功能模块：

1. **感知（Perception）**：从环境中获取信息并将其转化为内部表征。在水利认知智能系统中，感知对应于多源数据采集与预处理——SCADA信号、文本文档、图像和语音信息的接收与初步结构化。

2. **记忆（Memory）**：存储和检索信息的能力。区分短时记忆（工作记忆）和长时记忆。水利认知智能系统中，短时记忆对应当前会话上下文和工作状态，长时记忆对应知识库、规程库和历史案例库。

3. **推理（Reasoning）**：基于已有知识和当前信息得出新结论的能力。包括演绎推理（从一般到特殊）、归纳推理（从特殊到一般）和类比推理（从相似到相似）。水利认知智能中的风险评估、方案比较和异常归因都属于推理任务。

4. **语言（Language）**：以符号系统进行信息编码、传递和理解的能力。水利认知智能系统需要理解调度规程的自然语言描述，也需要以自然语言向操作人员解释决策依据。

[表2-1: 认知功能与水利认知智能系统的映射]

| 认知功能 | 人类认知 | 水利认知智能系统 | 对应技术 |
|---------|---------|----------------|---------|
| 感知 | 视觉、听觉、触觉 | 多源数据采集与预处理 | SCADA接口、OCR、语音识别 |
| 记忆 | 短时记忆、长时记忆 | 会话上下文、知识库 | 向量数据库、知识图谱（ch03） |
| 推理 | 演绎、归纳、类比 | 风险评估、方案比较 | 推理链、规则引擎、LLM推理（ch04） |
| 语言 | 自然语言理解与生成 | 规程解析、建议生成 | NLP、LLM（ch04-ch05） |

### 2.1.2 认知架构理论

认知科学中，"认知架构"（Cognitive Architecture）是指描述认知系统整体结构和运作方式的计算模型。几个有代表性的认知架构对水利认知智能的设计具有启发意义：

**ACT-R（Adaptive Control of Thought-Rational）**（Anderson, 1993）：将认知过程分解为声明性知识（"知道什么"）和程序性知识（"知道怎么做"）。这一区分对水利认知智能系统的知识组织有直接借鉴意义——调度规程中的条款属于声明性知识，而"遇到某类告警时的处置流程"属于程序性知识。

**SOAR**（Laird, 2012）：强调"问题空间搜索"和"分块学习"。当系统遇到新问题时，通过在问题空间中搜索找到解决方案；成功的解决方案被"分块"存储，下次遇到相似问题时可直接调用。这与水利认知智能中"历史案例检索+经验复用"的思路一致。

**Global Workspace Theory（全局工作空间理论）**（Baars, 1988）：提出意识是一个"全局广播"机制——多个专门化的处理模块竞争进入全局工作空间，获胜的信息被广播给所有模块。这一思想启发了认知AI引擎的多模块协同架构：感知模块、知识检索模块、推理模块和生成模块需要一个协调机制来整合各自的输出。

[物理直觉] 调度员面对复杂工况时的认知过程，本质上就是一个"多源感知→知识检索→推理比较→语言表达"的串行-并行混合过程。认知智能系统试图将这个过程计算化、可复现化。

### 2.1.3 从人类认知到机器认知：边界与谨慎

必须清醒认识到，当前的认知智能技术（包括LLM）并非真正实现了类人认知。人类认知具备常识、直觉、情感和伦理判断等深层能力，而当前技术本质上仍是基于统计模式匹配的信息处理系统。

对水利认知智能而言，这一认识的实践意义在于：

- **不应期望认知AI引擎具备"理解"能力**：它擅长的是"模式匹配+知识检索+文本生成"，而非真正的理解和推理；
- **安全关键决策必须有人类确认**：认知AI引擎的"推理"可能在表面上看起来合理，但缺乏对物理世界因果关系的真正把握；
- **系统设计应将"知识外置化"**：通过RAG和知识图谱将领域知识显式管理，而非依赖模型内部的隐式表征，以提高可控性和可审计性。

---

## 2.2 知识表示：如何将水利知识形式化

知识表示（Knowledge Representation, KR）是认知智能的基石——只有将水利领域的知识以计算机可处理的形式组织起来，才能支持后续的检索、推理和生成。

### 2.2.1 知识的分类

按照知识的结构化程度和表达方式，可将水利领域知识分为以下类型：

**事实性知识（Factual Knowledge）**：关于世界状态的陈述。例如："胶东调水工程干线全长483km""3号闸门额定开度为2.5m"。这类知识通常以"实体-属性-值"三元组表示。

**规则性知识（Procedural Knowledge）**：描述操作规程和决策逻辑的知识。例如："当上游水位超过设计水位0.3m且持续时间超过30分钟时，应立即通知下游管理所并启动预泄方案"。这类知识通常以IF-THEN规则或决策树表示。

**概念性知识（Conceptual Knowledge）**：描述概念间关系和分类层次的知识。例如："分层分布式控制（HDC）是一种控制架构，包含设备层（L0）、本地控制层（L1）和协调层（L2）三个层级"。这类知识通常以本体（Ontology）或语义网络表示。

**经验性知识（Experiential Knowledge）**：从实践中积累的非形式化知识。例如："冬季冰期运行时，闸门动作速度应放慢，避免冰块卡阻"。这类知识通常以自然语言文本存储，是非结构化知识管理的重点对象。

[表2-2: 水利知识分类与表示方法]

| 知识类型 | 典型示例 | 表示方法 | 处理技术 |
|---------|---------|---------|---------|
| 事实性知识 | 工程参数、设备台账 | 三元组、属性表 | 数据库、知识图谱 |
| 规则性知识 | 调度规程、联锁逻辑 | IF-THEN规则、决策树 | 规则引擎、流程图 |
| 概念性知识 | 学科分类、架构层次 | 本体、语义网络 | 本体编辑器、推理机 |
| 经验性知识 | 运行经验、处置案例 | 自然语言文本 | RAG、LLM |

### 2.2.2 逻辑表示

逻辑表示是最古老也最严格的知识表示方法，以命题逻辑和一阶谓词逻辑为代表。

**命题逻辑**：用命题变量和逻辑连接词表示知识。例如：

$$p: \text{上游水位超标} \quad q: \text{启动预泄方案}$$
$$p \rightarrow q \quad \text{（如果上游水位超标，则启动预泄方案）}$$

命题逻辑简单直观，但表达能力有限——无法表示个体、属性和量化关系。

**一阶谓词逻辑**：引入个体变量、谓词和量词，表达能力大幅提升。例如：

$$\forall x \, [\text{Gate}(x) \wedge \text{WaterLevel}(x) > \text{DesignLevel}(x) + 0.3 \rightarrow \text{ShouldAlert}(x)]$$

[工程解释] 该公式表示"对于所有闸门$x$，如果其水位超过设计水位0.3m，则应发出告警"。一阶逻辑在理论上可以表示任意复杂的水利知识，但在实践中面临两个问题：①将自然语言规程转化为逻辑公式的成本很高；②在不完全信息下的推理（如不确定性处理）能力有限。

### 2.2.3 语义网络

语义网络（Semantic Network）用节点表示概念或实体，用有向边表示概念间的关系。它比逻辑表示更直观，更易于人类理解和维护。

水利领域的语义网络示例：

```
[闸门] --is-a--> [水工建筑物]
[闸门] --has-property--> [开度]
[闸门] --has-property--> [设计流量]
[闸门] --located-in--> [渠段]
[渠段] --upstream-of--> [渠段]
[告警] --triggered-by--> [水位越限]
[告警] --requires-action--> [处置方案]
```

语义网络的优势在于直观性和灵活性，但缺乏严格的形式化语义定义——不同人构建的语义网络可能对同一关系有不同理解。这一问题催生了本体（Ontology）的发展。

### 2.2.4 本体

本体（Ontology）是对特定领域中概念、关系和约束的共享的、形式化的、显式的规范说明（Gruber, 1993）。它在语义网络的基础上增加了：

- **类（Class）层次结构**：定义概念的上下位关系。例如：水工建筑物 → 闸门 → 平板闸门/弧形闸门；
- **属性（Property）定义**：明确每个类可以拥有的属性及其值域。例如：闸门.开度 的值域为 [0, 最大开度] 的实数；
- **约束（Constraint）**：定义属性间的逻辑约束。例如：闸门.实际开度 ≤ 闸门.最大开度；
- **实例（Instance）**：本体的具体化。例如："3号闸"是"平板闸门"类的一个实例。

本体的标准描述语言包括OWL（Web Ontology Language）和RDF Schema。水利领域本体的构建方法将在第三章详细论述。

[表2-3: 知识表示方法比较]

| 方法 | 表达能力 | 可读性 | 推理支持 | 构建成本 | 水利适用场景 |
|------|---------|--------|---------|---------|------------|
| 命题逻辑 | 低 | 中 | 完备 | 低 | 简单联锁规则 |
| 一阶逻辑 | 高 | 低 | 半可判定 | 高 | 复杂规程形式化 |
| 语义网络 | 中 | 高 | 有限 | 中 | 概念关系可视化 |
| 本体(OWL) | 高 | 中 | 可判定子集 | 高 | 领域知识标准化 |
| 知识图谱 | 高 | 高 | 与推理引擎结合 | 中-高 | 综合知识管理（ch03） |

### 2.2.5 从知识表示到知识图谱

知识图谱（Knowledge Graph, KG）可以视为语义网络和本体的工程化实现。它以"实体-关系-实体"三元组（或"主语-谓语-宾语"三元组）为基本数据结构，结合图数据库存储和图查询语言（如SPARQL、Cypher），构建大规模的结构化知识网络。

知识图谱与传统本体的主要区别在于：
- 规模更大：知识图谱可包含数十亿三元组，覆盖广泛的领域知识；
- 容错性更强：允许知识的不完整和不一致，通过概率推理处理不确定性；
- 与NLP技术深度结合：知识图谱的构建（知识抽取）和应用（知识增强生成）都依赖NLP技术。

水利知识图谱的设计与构建将在第三章系统论述。本节仅建立"知识表示→知识图谱"的概念衔接。

---

## 2.3 自然语言处理：从规则到预训练

自然语言处理（Natural Language Processing, NLP）是认知智能的核心使能技术。本节概述NLP的关键技术演进，为理解LLM在水利领域的应用奠定基础。

### 2.3.1 NLP的核心任务

与水利认知智能直接相关的NLP任务包括：

**文本分类**：判断文档或段落的类别。例如，将告警文本分类为"水位告警""设备故障告警""通信异常告警"等。

**命名实体识别（Named Entity Recognition, NER）**：从文本中识别特定类型的实体。例如，从调度规程中识别出"3号闸门""引黄济青渠段""50m³/s"等实体及其类型（设备、位置、流量值）。

**关系抽取（Relation Extraction）**：从文本中识别实体间的关系。例如，从"3号闸门位于引黄济青渠段K32+500处"中抽取关系（3号闸门, 位于, K32+500）。

**文本摘要**：对长文本生成简短摘要。例如，为值班交接自动生成"过去8小时运行摘要"。

**问答（Question Answering）**：根据给定的知识源回答自然语言问题。例如，回答"当前工况下，3号闸门的安全开度范围是多少？"

**文本生成**：根据给定的条件和约束生成自然语言文本。例如，生成"可审计建议包"中的自然语言解释。

### 2.3.2 技术演进：四个阶段

**第一阶段：基于规则的方法（1950s-1990s）**

早期NLP完全依赖人工编写的语法规则和词典。例如，用正则表达式匹配"水位+数字+m"来提取水位值，用句法分析树解析规程条款的语义结构。

这种方法的优点是精确可控，缺点是覆盖面窄、维护成本高。水利领域的早期专家系统（参见ch01 §1.5.1）大多采用这种方法。

**第二阶段：统计方法（1990s-2010s）**

引入统计机器学习方法后，NLP不再依赖手工规则，而是从标注数据中自动学习模式。代表性技术包括：

- **隐马尔可夫模型（HMM）**：用于词性标注和命名实体识别；
- **条件随机场（CRF）**：用于序列标注任务，比HMM能捕捉更复杂的上下文特征；
- **TF-IDF + 朴素贝叶斯/SVM**：用于文本分类。

统计方法的核心贡献是证明了"数据驱动优于规则驱动"的范式，但其局限性在于需要大量标注数据和手工特征工程。

**第三阶段：深度学习方法（2013-2018）**

深度学习的引入带来了两个关键突破：

- **词向量（Word Embeddings）**：Word2Vec（Mikolov et al., 2013）和GloVe（Pennington et al., 2014）将词语映射到低维连续向量空间，使得语义相似的词在向量空间中距离相近。例如，"闸门"和"水闸"的向量距离会比"闸门"和"水泵"更近。

- **序列模型**：RNN、LSTM、GRU等循环神经网络能够处理变长序列，捕捉文本中的长距离依赖关系。BiLSTM+CRF成为NER任务的标准架构。

深度学习方法大幅减少了特征工程的工作量，但仍然需要针对每个任务分别训练模型。

**第四阶段：预训练语言模型（2018-至今）**

2018年，BERT（Bidirectional Encoder Representations from Transformers）（Devlin et al., 2019）的提出标志着NLP进入预训练时代。核心思想是：

1. **预训练（Pre-training）**：在大规模无标注文本上，通过自监督任务（如掩码语言模型）学习通用的语言表征；
2. **微调（Fine-tuning）**：在特定任务的少量标注数据上调整模型参数，使其适应具体任务。

这一范式的革命性在于：一个预训练模型可以通过微调适应多种下游任务，大幅降低了对标注数据的需求。随后，GPT系列模型进一步将这一范式推向"大语言模型"时代。

[表2-4: NLP技术演进四阶段]

| 阶段 | 时间 | 代表技术 | 核心创新 | 局限 |
|------|------|---------|---------|------|
| 规则方法 | 1950s-1990s | 正则表达式, CFG | 精确可控 | 覆盖面窄、维护成本高 |
| 统计方法 | 1990s-2010s | HMM, CRF, SVM | 数据驱动 | 需大量标注+手工特征 |
| 深度学习 | 2013-2018 | Word2Vec, LSTM | 自动特征学习 | 每任务单独训练 |
| 预训练模型 | 2018- | BERT, GPT, LLM | 预训练-微调范式 | 计算成本高、幻觉风险 |

### 2.3.3 词向量与语义表示

词向量（Word Embedding）是现代NLP的基础概念之一，对理解后续的LLM和RAG技术至关重要。

**基本思想**：将每个词映射到一个$d$维实数向量（通常$d$=100-1024），使得语义相似的词在向量空间中距离相近。

[物理直觉] 可以类比水力学中的状态空间：就像一个渠段的状态可以用水位$h$和流量$Q$两个数值描述一样，一个词的"语义状态"可以用$d$个数值描述。词向量空间中的距离反映了词义的相似度。

**Word2Vec的核心思想**：一个词的含义由它的上下文决定（分布假说）。Word2Vec通过两种训练方式学习词向量：

- **CBOW（Continuous Bag of Words）**：用上下文词预测中心词；
- **Skip-gram**：用中心词预测上下文词。

训练完成后，词向量具备有趣的代数性质。例如：

$$\vec{v}(\text{国王}) - \vec{v}(\text{男人}) + \vec{v}(\text{女人}) \approx \vec{v}(\text{女王})$$

在水利领域，类似的关系也可能存在：

$$\vec{v}(\text{闸门}) - \vec{v}(\text{明渠}) + \vec{v}(\text{管网}) \approx \vec{v}(\text{阀门})$$

**从词向量到句向量**：词向量描述单个词的语义，但水利认知智能需要处理的往往是句子、段落甚至文档级别的语义。句向量（Sentence Embedding）将整个句子映射到一个向量，使得语义相似的句子在向量空间中距离相近。这是RAG技术中"语义检索"的基础——将用户查询和知识库文档都映射到同一向量空间，通过向量距离找到最相关的文档片段。

---

## 2.4 Transformer架构

Transformer（Vaswani et al., 2017）是当前所有大语言模型的基础架构。理解Transformer的核心机制，对于理解LLM在水利领域的能力和局限至关重要。

### 2.4.1 从RNN到Transformer：为什么需要新架构

循环神经网络（RNN）及其变体LSTM/GRU是Transformer之前处理序列数据的主流架构。它们按时间步逐个处理输入序列中的元素，天然具备处理变长序列的能力。但RNN有两个根本性缺陷：

**缺陷一：长距离依赖衰减**。虽然LSTM通过门控机制缓解了梯度消失问题，但在处理很长的序列（如数千字的调度规程）时，序列开头的信息仍然难以有效传递到序列末尾。

**缺陷二：顺序计算瓶颈**。RNN必须按顺序处理序列中的每个元素，无法并行化。这使得训练大规模模型时的计算效率很低。

Transformer通过"自注意力机制"（Self-Attention）同时解决了这两个问题：它允许序列中的每个位置直接关注任意其他位置，既能捕捉长距离依赖，又能完全并行化计算。

### 2.4.2 自注意力机制

自注意力（Self-Attention）是Transformer的核心创新。其基本思想是：对于序列中的每个元素，计算它与序列中所有其他元素的"相关度"，然后根据相关度加权求和，得到该元素的新表示。

**计算过程**：

给定输入序列的表示矩阵 $\mathbf{X} \in \mathbb{R}^{n \times d}$（$n$个元素，每个$d$维），自注意力通过三个线性变换生成查询（Query）、键（Key）和值（Value）矩阵：

$$\mathbf{Q} = \mathbf{X}\mathbf{W}_Q, \quad \mathbf{K} = \mathbf{X}\mathbf{W}_K, \quad \mathbf{V} = \mathbf{X}\mathbf{W}_V$$

其中 $\mathbf{W}_Q, \mathbf{W}_K \in \mathbb{R}^{d \times d_k}$，$\mathbf{W}_V \in \mathbb{R}^{d \times d_v}$ 为可学习参数矩阵。

注意力权重和输出计算为：

$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}$$

[物理直觉] 可以用水力学中的类比来理解自注意力：想象一个水网中的每个节点都在"询问"（Query）其他节点的状态，每个节点同时"展示"（Key）自己的特征并"提供"（Value）自己的信息。注意力权重决定了每个节点从其他节点获取多少信息——就像水流按照水力梯度从高处流向低处，信息也按照"注意力梯度"从相关节点流向目标节点。

[工程解释] 分母中的$\sqrt{d_k}$是缩放因子，防止当$d_k$较大时点积值过大导致softmax输出趋向极端值（接近0或1），这与控制系统中的"增益归一化"类似。

【例2-1】自注意力的简化数值计算

假设输入序列包含3个词："闸门""水位""超标"，每个词用2维向量表示（实际维度远大于此，此处简化以展示计算过程）：

$$\mathbf{X} = \begin{bmatrix} 1.0 & 0.5 \\ 0.8 & 1.2 \\ 0.3 & 0.9 \end{bmatrix}$$

设变换矩阵 $\mathbf{W}_Q = \mathbf{W}_K = \mathbf{W}_V = \mathbf{I}$（单位阵，简化），则 $\mathbf{Q} = \mathbf{K} = \mathbf{V} = \mathbf{X}$。

[求解] 第一步，计算注意力分数矩阵 $\mathbf{Q}\mathbf{K}^T / \sqrt{d_k}$，其中 $d_k = 2$：

$$\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{2}} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1.25 & 1.40 & 0.75 \\ 1.40 & 2.08 & 1.32 \\ 0.75 & 1.32 & 0.90 \end{bmatrix}$$

第二步，对每行做softmax归一化，得到注意力权重矩阵。以第一行为例，softmax后"闸门"对三个词的注意力权重约为 (0.30, 0.42, 0.28)，说明"闸门"最关注"水位"。

[结果讨论] 在这个简化例子中，"闸门"和"水位"之间的注意力权重最高，符合水利文本中"闸门"常与"水位"共现的语义关联。实际LLM中，变换矩阵$\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V$通过训练学习，能够捕捉更丰富的语义关系。

### 2.4.3 多头注意力

单一的注意力头只能捕捉一种类型的关联模式。多头注意力（Multi-Head Attention）使用$h$个独立的注意力头，每个头捕捉不同类型的关联，然后将所有头的输出拼接并线性变换：

$$\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}_O$$

其中每个头 $\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_Q^i, \mathbf{K}\mathbf{W}_K^i, \mathbf{V}\mathbf{W}_V^i)$。

[工程解释] 在水利文本处理中，不同的注意力头可能分别关注不同类型的信息：一个头可能关注"实体-属性"关系（如"3号闸门"和"2.5m"的关联），另一个头可能关注"条件-动作"关系（如"水位超标"和"启动预泄"的关联），还有一个头可能关注"时间-事件"关系。

### 2.4.4 位置编码

自注意力机制本身是"位置无关"的——它不区分序列中元素的先后顺序。但文本的含义高度依赖于词序（"闸门控制水位"和"水位控制闸门"含义完全不同）。因此，Transformer需要额外的位置编码（Positional Encoding）来注入位置信息。

原始Transformer使用正弦/余弦函数生成位置编码：

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$

其中$pos$为位置索引，$i$为维度索引。

后续的LLM研究提出了多种改进的位置编码方案，如旋转位置编码（RoPE）（Su et al., 2021）、ALiBi（Press et al., 2022）等，它们能够更好地处理长序列——这对处理长篇调度规程尤为重要。

### 2.4.5 Transformer的完整架构

完整的Transformer由编码器（Encoder）和解码器（Decoder）两部分组成：

**编码器**：由$N$个相同的层堆叠而成，每层包含：
1. 多头自注意力子层
2. 前馈神经网络（FFN）子层
3. 每个子层后接残差连接和层归一化

**解码器**：同样由$N$个相同的层堆叠，每层包含：
1. 掩码多头自注意力子层（防止"看到"未来位置）
2. 编码器-解码器注意力子层
3. 前馈神经网络子层
4. 每个子层后接残差连接和层归一化

在后续的LLM发展中，不同模型选择了不同的架构变体：

| 架构变体 | 代表模型 | 特点 | 典型应用 |
|---------|---------|------|---------|
| 仅编码器 | BERT | 双向上下文理解 | 分类、NER、语义相似度 |
| 仅解码器 | GPT系列 | 自回归生成 | 文本生成、对话、推理 |
| 编码器-解码器 | T5, BART | 序列到序列转换 | 翻译、摘要、问答 |

[图2-1: Transformer架构示意图]
{描述: 左侧为编码器堆栈（N层），右侧为解码器堆栈（N层）。每层内部展示多头注意力、前馈网络、残差连接和层归一化的组合方式。编码器和解码器之间通过交叉注意力连接。}
{尺寸: 全页}
{颜色方案: 蓝色系}

---

## 2.5 大语言模型（LLM）

### 2.5.1 从BERT到GPT：预训练范式的演进

**BERT（2018）**：使用掩码语言模型（Masked Language Model, MLM）预训练——随机遮盖输入文本中15%的词，训练模型预测被遮盖的词。由于同时利用了左右两个方向的上下文，BERT擅长文本理解任务（分类、NER、语义匹配），但不擅长文本生成。

**GPT系列（2018-至今）**：使用自回归语言模型（Autoregressive LM）预训练——根据前面的所有词预测下一个词。GPT-2（Radford et al., 2019）展示了大规模预训练后的零样本学习能力；GPT-3（Brown et al., 2020）展示了少样本学习能力（In-Context Learning）；GPT-4及后续模型在推理、代码生成等方面进一步突破。

**规模定律（Scaling Laws）**：Kaplan et al.（2020）发现，语言模型的性能随模型参数量、训练数据量和计算量呈幂律关系提升。这一发现驱动了"越大越好"的模型扩展趋势。

### 2.5.2 LLM的核心能力

当前LLM展现的核心能力包括：

**语言理解**：理解复杂的自然语言输入，包括隐含意义、上下文依赖和修辞手法。例如，理解"上游来水压力较大"中"压力"是指流量偏大，而非物理压强。

**知识回忆**：在预训练过程中，LLM从大规模文本中"记忆"了大量世界知识。但这些知识的准确性和时效性无法保证——这是RAG技术存在的核心理由。

**推理能力**：LLM展现了一定的逻辑推理、数学计算和因果推理能力，尤其在结合"思维链"（Chain-of-Thought, CoT）提示时。但其推理的可靠性仍然不如符号推理系统。

**文本生成**：根据给定的上下文和指令生成连贯、流畅的自然语言文本。这是认知AI引擎生成"可审计建议包"的技术基础。

**少样本学习（In-Context Learning）**：通过在输入中提供少量示例，LLM能够快速适应新任务，无需重新训练。这对水利领域数据稀缺的场景尤为有价值。

### 2.5.3 涌现能力

"涌现能力"（Emergent Abilities）是指LLM在规模超过某个阈值后突然展现出小模型不具备的能力（Wei et al., 2022）。典型的涌现能力包括：

- **思维链推理**：当模型规模足够大时，在提示中加入"Let's think step by step"可显著提升推理任务的准确率；
- **指令遵循**：大模型能够理解并遵循复杂的自然语言指令，无需专门训练；
- **代码生成与执行**：大模型能够根据自然语言描述生成可执行代码。

[工程解释] 对水利认知智能而言，涌现能力意味着：①模型选择应优先考虑足够大的模型，即使推理成本更高；②提示词（Prompt）的设计对输出质量有显著影响——这将在第六章（认知AI引擎设计）中详细讨论。

### 2.5.4 LLM的局限性与风险

LLM在水利认知智能应用中面临以下固有局限：

**幻觉（Hallucination）**：LLM可能生成看似合理但事实上错误的内容。例如，引用不存在的调度规程条款、给出错误的工程参数、或编造不存在的历史案例。在安全关键的水利场景中，幻觉是最严重的风险。

**知识截止（Knowledge Cutoff）**：LLM的知识来源于预训练数据，无法获取训练截止日期之后的信息。调度规程的更新、设备的更换和工程的改造都不在模型的知识范围内。

**不可解释性**：LLM的决策过程发生在数十亿参数的黑箱中，无法直接追溯"为什么生成了这个回答"。这与水利认知智能对"可审计"的要求存在张力——解决这一张力的方法是通过RAG和知识图谱实现"白箱输出"（参见ch01习题1-6的讨论）。

**一致性问题**：同一个问题，LLM在不同时间或不同表述下可能给出不同甚至矛盾的回答。在需要严格一致性的调度场景中，这是不可接受的。

**安全对齐**：LLM可能被误导性输入"诱导"生成不当内容（提示注入攻击）。在水利SCADA系统中，如果认知AI引擎被攻击者通过构造输入操控，可能产生安全风险。

[表2-5: LLM能力与局限的对应关系]

| 能力 | 对应水利应用 | 局限 | 应对策略 |
|------|------------|------|---------|
| 语言理解 | 规程解析、告警理解 | 可能曲解专业术语 | 领域微调+术语词典 |
| 知识回忆 | 常识推理辅助 | 知识过时或错误 | RAG外挂知识库（ch05） |
| 推理能力 | 风险评估、方案比较 | 推理不可靠 | CoT提示+规则校验（ch06） |
| 文本生成 | 建议包、摘要生成 | 幻觉 | 引用追溯+人工确认（ch07） |
| 少样本学习 | 快速适配新工况 | 示例质量敏感 | 提示工程+评估基准 |

---

## 2.6 预训练-微调-对齐：LLM训练三阶段

### 2.6.1 预训练阶段

预训练阶段的目标是让模型学习通用的语言表征和世界知识。

**训练数据**：通常包含数万亿Token的文本数据，来源于网页、书籍、学术论文、代码等。数据的质量和多样性对模型性能有决定性影响。

**训练目标**：对于自回归模型（如GPT），训练目标是最大化下一个Token的预测概率：

$$\mathcal{L}(\theta) = -\sum_{t=1}^{T} \log P(x_t \mid x_1, \ldots, x_{t-1}; \theta)$$

其中$\theta$为模型参数，$x_1, \ldots, x_T$为输入Token序列。

[工程解释] 这个目标函数的含义是：给定前面所有的词，模型应尽可能准确地预测下一个词。通过在海量文本上优化这个目标，模型隐式地学习了语法、语义、事实知识和一定的推理能力。

### 2.6.2 监督微调阶段（SFT）

预训练后的模型虽然具备强大的语言能力，但其行为方式可能不符合用户期望——例如，可能生成冗长、不相关或不安全的回答。监督微调（Supervised Fine-Tuning, SFT）通过在高质量的"指令-回答"数据对上继续训练，使模型学会遵循指令并生成有用、相关的回答。

对水利领域而言，SFT数据可以包括：
- 调度规程相关的问答对；
- 工况分析与建议生成的示例；
- 告警处置流程的描述与执行步骤。

第四章将详细讨论水利领域LLM微调的数据准备和训练策略。

### 2.6.3 对齐阶段（RLHF/DPO）

对齐（Alignment）阶段的目标是使模型的行为与人类偏好一致。两种主流方法：

**RLHF（Reinforcement Learning from Human Feedback）**（Ouyang et al., 2022）：
1. 收集人类对多个模型回答的偏好排序；
2. 训练一个奖励模型（Reward Model）预测人类偏好；
3. 使用PPO等强化学习算法优化LLM，使其输出获得更高的奖励。

**DPO（Direct Preference Optimization）**（Rafailov et al., 2023）：直接从偏好数据优化模型，不需要单独训练奖励模型，简化了训练流程。

[工程解释] 对齐对水利认知智能尤其重要：需要确保模型"拒绝"生成越过安全联锁的直接执行指令，"偏好"生成带有证据引用的建议，"避免"在不确定时编造答案。这些偏好需要通过领域特定的对齐训练来实现。

[图2-2: LLM训练三阶段流程]
{描述: 从左到右三个阶段的流程图。预训练阶段：大规模文本→语言模型；SFT阶段：指令-回答数据→指令模型；对齐阶段：人类偏好数据→对齐模型。每个阶段标注输入数据类型、训练目标和输出能力。}
{尺寸: 全页}
{颜色方案: 蓝色系}

---

## 2.7 信息检索与向量数据库

RAG技术的基础是信息检索（Information Retrieval, IR）。本节介绍与RAG直接相关的检索技术基础。

### 2.7.1 稀疏检索与稠密检索

**稀疏检索**：基于词频统计的传统检索方法，代表是BM25算法。BM25通过计算查询和文档之间的词项匹配度来排序。对于水利领域的精确术语匹配（如查询特定闸门编号、特定规程条款号），稀疏检索仍然非常有效。

**稠密检索**：使用预训练语言模型将查询和文档编码为稠密向量，通过向量相似度（如余弦相似度）进行检索。稠密检索的优势在于语义匹配——即使查询和文档使用不同的词汇表述同一概念，也能成功匹配。

例如，查询"闸门开度过大导致下游水位偏高"与文档"过流量超出设计值引发下游壅水"在词汇层面差异很大，但在语义层面高度相关。稀疏检索可能漏检这篇文档，而稠密检索能够正确匹配。

**混合检索**：结合稀疏和稠密检索的优势，先分别计算两种方法的得分，再加权融合。在水利场景中，混合检索通常是最佳选择——既保证精确术语的可靠匹配，又支持语义层面的模糊匹配。

### 2.7.2 向量数据库

向量数据库（Vector Database）是专门用于存储和检索高维向量的数据库系统。常见的向量数据库包括Milvus、Pinecone、Weaviate、FAISS等。

向量数据库的核心操作是**近邻搜索（Nearest Neighbor Search）**：给定一个查询向量，在数据库中找到与之最相似的$k$个向量。精确近邻搜索的计算复杂度为$O(n)$，其中$n$为数据库中的向量数量。对于大规模数据库，通常使用近似最近邻（Approximate Nearest Neighbor, ANN）算法，如HNSW（Hierarchical Navigable Small World）、IVF（Inverted File）等，将搜索复杂度降低到$O(\log n)$级别。

[工程解释] 在水利认知智能系统中，向量数据库的典型用法是：将调度规程、事故通报等文档切分为段落，每个段落编码为一个向量存入数据库。当调度员提问时，将问题编码为查询向量，在数据库中检索最相关的段落，作为LLM生成回答的证据来源。这正是RAG技术的核心流程，将在第五章详细论述。

### 2.7.3 向量数据库选型建议

对于水利认知智能系统的工程部署，常见的向量数据库方案对比如下：

| 方案 | 类型 | 适用规模 | 部署方式 | 适用场景 |
|------|------|---------|---------|---------|
| FAISS | 库（非数据库） | 百万-十亿向量 | 嵌入应用进程 | 离线检索、原型验证 |
| Milvus | 分布式向量数据库 | 十亿级以上 | 独立集群部署 | 生产环境大规模检索 |
| Chroma | 轻量级向量数据库 | 百万级以下 | 嵌入应用或单机 | 快速原型、小规模试点 |
| pgvector | PostgreSQL扩展 | 百万级 | 复用现有PG实例 | 已有PG基础设施的团队 |

对于水利行业的典型部署场景，建议分阶段选型：试点阶段使用Chroma或FAISS快速验证效果，生产阶段根据数据规模选择Milvus（大规模）或pgvector（中等规模且已有PostgreSQL基础设施）。

---

## 2.8 评估方法与基准

### 2.8.1 NLP任务的通用评估指标

| 任务类型 | 常用指标 | 说明 |
|---------|---------|------|
| 分类 | Accuracy, Precision, Recall, F1 | 适用于告警分类等 |
| NER | Entity-level F1 | 严格匹配实体边界和类型 |
| 文本生成 | BLEU, ROUGE, BERTScore | 自动评估生成质量 |
| 问答 | Exact Match (EM), F1 | 抽取式问答的标准指标 |
| 检索 | Recall@K, MRR, nDCG | 评估检索结果的相关性 |

### 2.8.2 水利领域的评估特殊性

通用NLP评估指标在水利领域的应用需要注意以下特殊性：

**安全性评估优先**：在水利认知智能中，"安全"比"准确"更重要。一个回答即使在BLEU分数上不够高，只要不包含危险建议就可以接受；反之，一个流畅但包含错误安全建议的回答是不可接受的。需要设计专门的安全性评估指标和测试集。

**领域准确性**：通用的Accuracy指标无法区分"专业术语使用错误"和"措辞不够优雅"。水利认知智能的评估需要领域专家参与，构建包含正确/错误专业表述的测试集。

**引用可追溯性**：RAG系统的评估不仅要看回答是否正确，还要看回答是否引用了正确的来源。"答对了但引用错误"和"答对了且引用正确"是不同的质量等级。

**实时性**：认知AI引擎的响应时间也是评估的重要维度。在防洪调度场景中，响应时间超过60秒可能失去实用价值。

[表2-6: 水利认知智能评估维度]

| 评估维度 | 指标示例 | 权重建议 | 说明 |
|---------|---------|---------|------|
| 安全性 | 危险建议率、安全合规率 | ★★★★★ | 一票否决 |
| 领域准确性 | 术语正确率、参数合理性 | ★★★★☆ | 需领域专家评估 |
| 引用可追溯性 | 引用准确率、来源覆盖率 | ★★★★☆ | RAG系统核心指标 |
| 响应质量 | BLEU, ROUGE, 人工评分 | ★★★☆☆ | 自动+人工结合 |
| 响应时间 | P50/P95延迟 | ★★★☆☆ | 场景相关 |

---

## 本章小结

本章系统介绍了水利认知智能的四个理论基础：

1. **认知科学基础**：认知科学将认知过程分解为感知、记忆、推理和语言四大功能，这四大功能可以映射到水利认知智能系统的对应模块。认知架构理论（ACT-R、SOAR、全局工作空间理论）为系统设计提供了概念框架。同时，必须清醒认识到当前技术与人类认知之间的本质差距，避免过度拟人化的期望。

2. **知识表示**：水利领域知识可分为事实性、规则性、概念性和经验性四类。从逻辑表示到语义网络再到本体，知识表示方法在表达能力和工程可用性之间不断权衡。知识图谱作为工程化的知识表示方案，是水利认知智能的核心基础设施（ch03将详细论述）。

3. **自然语言处理演进**：NLP经历了规则方法→统计方法→深度学习→预训练模型四个阶段。词向量和Transformer架构是理解现代NLP的两个关键概念。Transformer通过自注意力机制解决了RNN的长距离依赖和并行化问题。

4. **大语言模型**：LLM通过预训练-微调-对齐三阶段训练获得强大的语言理解和生成能力，但同时面临幻觉、知识截止、不可解释性和安全对齐等固有局限。这些局限决定了水利认知智能不能简单依赖LLM，而必须结合RAG（ch05）、知识图谱（ch03）和安全约束框架（参见M3）构建完整的技术体系。

本章建立的理论框架将在后续章节中持续引用：ch03将展开知识表示方法的水利领域应用，ch04将深入Transformer/LLM的领域适配方法，ch05将基于信息检索理论构建水利RAG系统。

---

## 习题

### 基础题

**2-1.** 简述认知科学中"感知—记忆—推理—语言"四大功能，并各举一个水利认知智能系统中的具体应用场景。

**2-2.** 比较逻辑表示、语义网络和本体三种知识表示方法的优缺点。在水利调度规程的形式化表示中，你认为哪种方法最为适合？为什么？

**2-3.** Transformer架构中自注意力机制的计算公式是什么？缩放因子$\sqrt{d_k}$的作用是什么？

**2-4.** 列出大语言模型训练的三个阶段及其各自的训练目标。

### 应用题

**2-5.** 某水利管理单位拥有以下知识资产：①调度规程PDF文档50份；②设备台账Excel表格200份；③历史事故复盘报告100份；④实时SCADA数据流。请设计一个知识表示方案，说明每类知识资产适合使用哪种表示方法（三元组/规则/本体/自然语言），并解释理由。

**2-6.** 假设你需要为一个水利调度场景选择NLP技术方案。已知需求包括：①从规程中提取操作条件和动作（NER+关系抽取）；②将用户问题与规程段落匹配（语义检索）；③生成自然语言解释（文本生成）。请为每个需求推荐合适的技术方案（传统方法或LLM），并说明选择理由。

### 思考题

**2-7.** 大语言模型的"涌现能力"是否意味着只要模型足够大，就能解决水利认知智能的所有问题？请从数据特殊性、安全约束和可解释性三个角度论述规模扩展的局限性。

---

## 拓展阅读

1. **Vaswani, A. et al. (2017).** "Attention Is All You Need." *NeurIPS 2017*. — Transformer架构的原始论文，现代NLP和LLM的基石。

2. **Devlin, J. et al. (2019).** "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." *NAACL 2019*. — 预训练语言模型的里程碑工作。

3. **Brown, T.B. et al. (2020).** "Language Models are Few-Shot Learners." *NeurIPS 2020*. — GPT-3论文，展示了大规模语言模型的少样本学习能力。

4. **Ouyang, L. et al. (2022).** "Training Language Models to Follow Instructions with Human Feedback." *NeurIPS 2022*. — RLHF对齐方法的代表性论文。

5. **Gruber, T.R. (1993).** "A Translation Approach to Portable Ontology Specifications." *Knowledge Acquisition*, 5(2), 199-220. — 本体的经典定义论文。

6. **Mikolov, T. et al. (2013).** "Efficient Estimation of Word Representations in Vector Space." *ICLR 2013*. — Word2Vec原始论文，词向量技术的奠基工作。

7. **Wei, J. et al. (2022).** "Emergent Abilities of Large Language Models." *Transactions on Machine Learning Research*. — LLM涌现能力的系统分析。

---

> **下一章预告**：第三章将从理论走向实践，系统论述水利领域知识图谱的构建方法。包括水利本体设计、知识抽取技术（从规程文本中自动提取实体和关系）、知识图谱存储与查询，以及知识图谱在水利认知智能系统中的应用模式。
