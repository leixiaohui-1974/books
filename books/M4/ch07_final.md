<!-- 变更日志
v1 2026-02-19: 初稿（基于v1草稿重写，合规化）
-->

# 第七章 认知增强与在线学习

## 学习目标

1. 理解MAS中"可学习Agent"的概念——Agent在运行中持续优化策略的能力及其价值；
2. 掌握强化学习（RL）与安全约束结合的方法，理解"安全强化学习"在水网MAS中的应用框架；
3. 理解经验共享机制——Agent如何将个体学习成果安全地传播到其他Agent；
4. 掌握认知AI（大语言模型）与MAS融合的架构模式及其在知识推理和异常诊断中的应用；
5. 理解"可学习但不失控"的学习治理框架。

> **前章回顾**：第六章建立了MAS在环验证方法，通过四类交互测试、通信故障注入和一致性收敛测试确保了MAS部署前的安全性。但部署后的运行环境是动态变化的——水文条件变化、设备老化、用户需求演变。如果Agent的策略是固定的，系统性能将逐渐退化。本章讨论如何让Agent在运行中学习和优化，同时确保学习过程不违反安全约束。

---

## 可学习Agent的概念

### 7.1.1 为什么Agent需要学习

水利控制环境的非平稳性要求Agent具备学习能力：渠道糙率随季节变化（淤积、植被）、设备特性随使用年限退化（闸门死区增大、泵效率下降）、用户用水模式随经济发展和气候变化演变。固定策略的Agent会随环境变化而性能退化。

可学习Agent在运行中持续优化两类知识：**模型知识**（对被控对象动态特性的认识，如糙率参数估计）和**策略知识**（在特定工况下的最优控制动作选择）。

### 7.1.2 学习的风险

学习带来的核心风险是不稳定性——学习过程中的探索行为可能违反安全约束。未经约束的在线学习可能导致：Agent在"尝试新策略"时触发安全联锁、Agent学到的"局部最优"策略与全局目标冲突、多个Agent同时学习导致系统行为不可预测。

因此，水网MAS的学习必须在严格的治理框架下进行——"可学习但不失控"。

---

## 安全强化学习

### 7.2.1 受约束MDP框架

水网MAS中的强化学习采用受约束马尔可夫决策过程（Constrained MDP, CMDP）框架。与标准MDP不同，CMDP在最大化累积奖励的同时，要求满足安全约束的期望值上界：

$$\max_\pi \mathbb{E}\left[\sum_{t=0}^T \gamma^t r(s_t, a_t)\right] \quad \text{s.t.} \quad \mathbb{E}\left[\sum_{t=0}^T \gamma^t c_i(s_t, a_t)\right] \leq d_i, \quad \forall i$$

其中$c_i$是第$i$个安全约束的代价函数，$d_i$是其上界。在水网控制中，约束包括水位不越限（$c_1$）、闸门动作频率不超限（$c_2$）、上下游流量协调偏差不超限（$c_3$）等。

### 7.2.2 安全层设计

为了在学习过程中硬性保证安全约束不被违反（而非仅在期望意义上满足），CHS引入"安全层"（Safety Layer）架构：RL智能体的输出动作在执行前经过安全层过滤——安全层检查该动作是否可能导致约束违反，如果是，则将动作投影到安全集合内。

**表7-1 安全强化学习的三层架构**

| 层次 | 功能 | 输入 | 输出 |
|------|------|------|------|
| 学习层 | RL策略优化 | 状态观测+奖励信号 | 候选动作$a_{raw}$ |
| 安全层 | 约束检查与投影 | $a_{raw}$+当前状态+安全包络 | 安全动作$a_{safe}$ |
| 执行层 | 动作下发 | $a_{safe}$ | 控制指令 |

安全层的设计依赖于第三章Agent五元组模型中的安全包络定义——红/黄/绿三区间的划分为安全层提供了明确的约束边界。

### 7.2.3 学习治理规则

【例7-1】某闸站Agent采用安全强化学习优化其PI控制参数。

[学习目标] 在保证水位安全的前提下，最小化闸门调节频率（减少设备磨损）。

[安全约束] 水位偏差不超过±0.15m（硬约束），闸门单次调节幅度≤0.3m（软约束）。

[学习配置]
- 探索策略：ε-贪婪，ε从0.2线性衰减到0.02（100个episode）
- 安全层：基于当前水位和流量的线性预测模型，预判下一步水位是否可能越限
- 更新频率：每24小时更新一次策略参数，更新前需经安全层验证

[学习结果] 经过30天学习，Agent将闸门日均调节次数从47次降低到31次（减少34%），同时水位控制精度保持在±0.12m以内。安全层在学习过程中共拦截了23次可能导致水位越限的候选动作。

[结果讨论] 安全层的价值在于：即使RL探索产生了"冒险"的动作，安全层也能将其拉回安全范围。但安全层的过度保守可能限制学习效率——如果安全层频繁拦截，RL可能无法充分探索策略空间。

---

## 经验共享机制

### 7.3.1 联邦学习在水网MAS中的应用

多个闸站Agent面临相似但不完全相同的控制问题。经验共享允许Agent将学习成果传播到其他Agent，加速整体学习速度。

水网MAS采用联邦学习（Federated Learning）范式：各Agent在本地数据上训练模型，仅将模型参数（而非原始数据）上传到协调Agent进行聚合，聚合后的全局模型再分发到各Agent。这种方式在保护数据隐私（各站运行数据不出站）的同时实现了知识共享。

### 7.3.2 经验共享的安全问题

经验共享的风险在于：某个Agent在异常环境下学到的"坏经验"可能通过共享机制传播到其他Agent。例如，一个传感器漂移导致Agent学到了错误的水位-流量关系，如果这一错误知识被传播，可能导致多个站点同时出现控制偏差。

防范措施包括：经验验证门禁（共享前在仿真环境中验证）、异常值检测（过滤偏离统计分布的模型参数）和渐进采纳（新经验的权重从小到大逐步增加）。

【例7-2】5个闸站Agent的联邦学习经验共享。

[配置] 5个Agent各自学习PI参数优化策略，每周聚合一次。

[异常场景] Agent-3的水位传感器在第2周出现0.05m系统偏差（未被检测），导致Agent-3学到的策略过于激进。

[无防护的共享] 第3周聚合后，全局模型受Agent-3影响，其他4个Agent的控制性能均下降约8%。

[有防护的共享] 经验验证门禁在MIL环境中测试Agent-3的贡献，发现其策略在标准场景下性能异常。Agent-3的贡献被标记为可疑并暂不采纳。后续排查发现传感器偏差，校准后恢复正常共享。

[结果讨论] 经验验证门禁作为"共享前的MIL测试"，将在环验证的理念延伸到了学习阶段。

---

## 认知AI与MAS融合

### 7.4.1 大语言模型在MAS中的角色

认知AI（以大语言模型为核心）在MAS中不直接参与控制决策，而是作为"知识推理引擎"辅助Agent处理非结构化问题：

**异常诊断**：当MAS检测到异常但无法通过规则匹配确定原因时，认知AI可综合多源信息（传感器数据、历史事件、设备档案）进行推理诊断。

**知识检索**：当Agent面临罕见工况（如从未经历的极端来水）时，认知AI可从运行规程、设计文档和历史案例中检索相关知识供Agent参考。

**协调辅助**：在多利益主体的协商中，认知AI可帮助生成协商方案、评估方案影响并解释决策理由。

### 7.4.2 认知AI的治理约束

**表7-2 认知AI在MAS中的权限边界**

| 认知AI功能 | 允许的权限 | 禁止的权限 |
|-----------|----------|----------|
| 异常诊断 | 提供诊断建议 | 直接修改控制参数 |
| 知识检索 | 检索和摘要信息 | 绕过安全联锁 |
| 协调辅助 | 生成备选方案 | 直接下达控制指令 |
| 学习辅导 | 推荐学习参数调整 | 覆盖安全层设置 |

核心原则：认知AI是"顾问"而非"执行者"——其输出必须经过Agent的决策逻辑和安全层过滤后才能影响实际控制。

---

## 学习治理框架

### 7.5.1 四层治理架构

"可学习但不失控"的治理框架包含四个层次：

**第一层·安全包络**：学习不得违反安全包络定义的硬约束（水位上下限、联锁触发条件）。这是不可协商的底线。

**第二层·ODD约束**：学习的探索范围不得超出ODD边界。如果学习过程导致系统状态接近ODD边界，探索应自动收缩。

**第三层·变更审计**：所有学习引起的策略变更必须被记录和可追溯。包括变更时间、变更内容、变更原因（哪些数据驱动了变更）和变更效果。

**第四层·回滚机制**：如果学习后的策略在运行中表现劣于学习前，系统应能自动回滚到学习前的策略版本。

---

## 本章小结

本章建立了水网MAS中"可学习但不失控"的在线学习框架。

首先，阐述了可学习Agent的概念和必要性——水利控制环境的非平稳性要求Agent具备持续学习能力，但学习的探索行为可能违反安全约束。

其次，介绍了安全强化学习的CMDP框架和三层架构（表7-1）——学习层负责策略优化，安全层负责约束过滤，执行层负责动作下发。例7-1展示了安全层在学习过程中拦截危险动作的实际效果。

第三，讨论了基于联邦学习的经验共享机制和安全防护措施。例7-2展示了经验验证门禁在防止"坏经验"传播中的关键作用。

第四，定义了认知AI在MAS中的角色和权限边界（表7-2）——认知AI是"顾问"而非"执行者"，其输出必须经过Agent决策逻辑和安全层过滤。

第五，建立了四层学习治理架构——安全包络、ODD约束、变更审计和回滚机制。

> 下一章将从技术转向工程管理——MAS的工程化运维需要组织架构与Agent权限的映射、版本治理、灰度发布和监控告警等配套机制。

---

## 习题

### 基础题

**7-1.** 解释"可学习但不失控"的含义。为什么水网MAS中的在线学习不能采用无约束的自由探索？

**7-2.** 比较标准MDP与受约束MDP（CMDP）的区别。在水利控制中，CMDP的约束$c_i$可能包含哪些物理量？

**7-3.** 解释安全层的"动作投影"机制。如果RL输出的候选动作$a_{raw}$会导致水位越限，安全层应如何处理？

**7-4.** 联邦学习与集中式学习的主要区别是什么？在水网MAS中采用联邦学习的主要动机是什么（数据隐私还是通信效率）？

### 应用题

**7-5.** 为一个3-Agent水网设计安全强化学习方案。要求：（a）定义状态空间（至少5个状态变量）；（b）定义动作空间；（c）定义奖励函数（至少包含控制精度和设备磨损两个目标）；（d）定义至少3个安全约束及其安全层实现方法。

**7-6.** 为上述3-Agent水网设计经验共享方案。要求：（a）确定共享内容（模型参数/策略参数/经验数据）；（b）确定共享频率和聚合方法；（c）设计经验验证门禁的测试用例（至少5个）；（d）描述异常Agent贡献的检测方法。

### 思考题

**7-7.** 安全层的保守程度是一个设计权衡：过于保守会限制学习效率（频繁拦截探索动作），过于宽松会增加安全风险。讨论：（a）如何量化安全层的保守程度？（b）是否可以随着Agent对环境的认知增加而逐步放宽安全层？（c）这种"动态安全边界"策略的风险是什么？

**7-8.** 认知AI（LLM）在当前技术水平下存在"幻觉"问题——可能生成看似合理但实际错误的信息。在水利MAS中，如果认知AI对异常诊断的建议是错误的，可能导致什么后果？如何设计MAS架构来缓解LLM幻觉风险？

---

## 拓展阅读

1. **Garcı́a, J. & Fernández, F. (2015).** "A comprehensive survey on safe reinforcement learning." *Journal of Machine Learning Research*, 16(1), 1437-1480. — 安全强化学习的综合综述，系统分类了安全约束的实现方法。

2. **McMahan, B., et al. (2017).** "Communication-efficient learning of deep networks from decentralized data." *AISTATS 2017*. — 联邦学习的基础论文（FedAvg算法），为本章经验共享机制提供了方法论基础。

3. **Lei, X. et al. (2025a).** "Cybernetics of Hydro Systems: Theory and Framework." *南水北调与水利科技（中英文）*. — CHS理论框架中关于认知增强原理的阐述。

4. **Brunke, L., et al. (2022).** "Safe learning in robotics: From learning-based control to safe reinforcement learning." *Annual Review of Control, Robotics, and Autonomous Systems*, 5, 411-444. — 安全学习的最新综述，对安全层和屏障函数方法有深入讨论。
