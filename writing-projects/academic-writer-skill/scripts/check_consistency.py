#!/usr/bin/env python3
"""
è·¨æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥è„šæœ¬
ç”¨æ³•: python3 check_consistency.py <æ–‡æ¡£ç›®å½•>
æŒ‰ SKILL.md Â§6 CONS-01~CONS-10 è¿›è¡Œäº¤å‰æ£€æŸ¥

æ£€æŸ¥é€»è¾‘ï¼šä»å·²å®Œæˆæ–‡æ¡£ä¸­æå–å…³é”®è¡¨è¿°ï¼Œæ£€æµ‹ä¸ä¸€è‡´ã€‚
"""
import re
import sys
import os
import json
from pathlib import Path
from collections import defaultdict

# ============================================================
# CONSè§„åˆ™å®šä¹‰
# ============================================================

CONS_RULES = {
    'CONS-01': {
        'name': 'CHSå…«åŸç†è¡¨è¿°',
        'keywords': ['å…«åŸç†', 'eight principles', 'CHSåŸç†', 'cybernetics principles'],
        'extract_pattern': r'((?:å…«|8|eight)\s*(?:åŸç†|principles|å¤§åŸç†).*?(?:\n\n|\Z))',
    },
    'CONS-02': {
        'name': 'WNAL(L0-L5)å®šä¹‰',
        'keywords': ['WNAL', 'L0', 'L1', 'L2', 'L3', 'L4', 'L5', 'è‡ªä¸»è¿è¡Œç­‰çº§', 'autonomy level'],
        'extract_pattern': r'((?:WNAL|æ°´ç½‘è‡ªä¸»è¿è¡Œç­‰çº§|Water Network Autonomy Level).*?(?:L5|Level\s*5).*?(?:\n\n|\Z))',
    },
    'CONS-03': {
        'name': 'Saint-Venantæ–¹ç¨‹ç¦»æ•£åŒ–è¡¨è¿°',
        'keywords': ['Saint-Venant', 'SVæ–¹ç¨‹', 'shallow water', 'æµ…æ°´æ–¹ç¨‹'],
        'extract_pattern': r'(Saint-Venant.*?(?:\n\n|\Z))',
    },
    'CONS-04': {
        'name': 'MPCæ§åˆ¶åŸç†æè¿°',
        'keywords': ['MPC', 'Model Predictive Control', 'æ¨¡å‹é¢„æµ‹æ§åˆ¶', 'DMPC'],
        'extract_pattern': r'((?:MPC|æ¨¡å‹é¢„æµ‹æ§åˆ¶|Model Predictive Control|DMPC).*?(?:\n\n|\Z))',
    },
    'CONS-05': {
        'name': 'HydroOSäº”å±‚æ¶æ„',
        'keywords': ['HydroOS', 'äº”å±‚', 'five-layer', 'æ°´ç½‘æ“ä½œç³»ç»Ÿ'],
        'extract_pattern': r'(HydroOS.*?(?:äº”å±‚|five.?layer|æ¶æ„|architecture).*?(?:\n\n|\Z))',
    },
    'CONS-06': {
        'name': 'èƒ¶ä¸œè°ƒæ°´å·¥ç¨‹å‚æ•°',
        'keywords': ['èƒ¶ä¸œ', 'Jiaodong', 'è°ƒæ°´'],
        'extract_pattern': r'(èƒ¶ä¸œ.*?(?:km|å…¬é‡Œ|mÂ³|è°ƒæ°´é‡|æµé‡).*?(?:\n\n|\Z))',
    },
    'CONS-07': {
        'name': 'æ²™åªæ°´ç”µç«™å·¥ç¨‹å‚æ•°',
        'keywords': ['æ²™åª', 'Shaping', 'æ°´ç”µç«™'],
        'extract_pattern': r'(æ²™åª.*?(?:MW|è£…æœº|æ°´å¤´|æµé‡).*?(?:\n\n|\Z))',
    },
    'CONS-08': {
        'name': 'å—æ°´åŒ—è°ƒä¸­çº¿å·¥ç¨‹å‚æ•°',
        'keywords': ['å—æ°´åŒ—è°ƒ', 'South-to-North', 'ä¸­çº¿', 'middle route'],
        'extract_pattern': r'(å—æ°´åŒ—è°ƒ.*?(?:km|å…¬é‡Œ|æ¸ æ± |1432).*?(?:\n\n|\Z))',
    },
    'CONS-09': {
        'name': 'å®‰å…¨åŒ…ç»œODDè¾¹ç•Œæ¡ä»¶',
        'keywords': ['ODD', 'Operational Design Domain', 'è¿è¡Œè®¾è®¡åŸŸ', 'å®‰å…¨åŒ…ç»œ', 'Safety Envelope'],
        'extract_pattern': r'((?:ODD|è¿è¡Œè®¾è®¡åŸŸ|Operational Design Domain|å®‰å…¨åŒ…ç»œ).*?(?:\n\n|\Z))',
    },
    'CONS-10': {
        'name': 'Â§5.1æœ¯è¯­è¡¨ä¸­è‹±å¯¹ç…§',
        'keywords': ['CHS', 'HydroOS', 'WNAL', 'IDZ', 'DMPC', 'MAS'],
        'check_type': 'terminology',  # ç‰¹æ®Šå¤„ç†ï¼šæ£€æŸ¥æœ¯è¯­å…¨ç§°æ˜¯å¦ä¸€è‡´
    },
}

# æœ¯è¯­æ ‡å‡†å®šä¹‰ï¼ˆæ¥è‡ªSKILL.md Â§5.1ï¼‰
STANDARD_TERMS = {
    'CHS': 'Cybernetics of Hydro Systems',
    'HydroOS': 'Hydraulic Network Operating System',
    'WNAL': 'Water Network Autonomy Level',
    'ODD': 'Operational Design Domain',
    'IDZ': 'Integrator Delay Zero',
    'DMPC': 'Distributed Model Predictive Control',
    'MAS': 'Multi-Agent System',
    'MIL': 'Model-in-the-Loop',
    'SIL': 'Software-in-the-Loop',
    'HIL': 'Hardware-in-the-Loop',
    'SE': 'Safety Envelope',
    'CI': 'Cognitive Intelligence',
    'HWLM': 'Hando Water Network Large Model',
}

# ============================================================
# æ–‡ä»¶æ‰«æ
# ============================================================

def find_documents(base_dir):
    """æ‰«æç›®å½•ä¸‹æ‰€æœ‰å·²å®Œæˆçš„æ–‡æ¡£"""
    docs = []
    base = Path(base_dir)
    
    for ext in ['*.md', '*.txt', '*.docx']:
        for f in base.rglob(ext):
            if f.name.startswith('.') or 'node_modules' in str(f):
                continue
            if any(skip in str(f) for skip in ['progress.json', 'review_', 'check_report', 'SKILL.md', 'CLAUDE.md']):
                continue
            docs.append(f)
    
    return docs

def read_document(filepath):
    """è¯»å–æ–‡æ¡£å†…å®¹"""
    path = Path(filepath)
    if path.suffix == '.md' or path.suffix == '.txt':
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            return ''
    elif path.suffix == '.docx':
        try:
            import docx
            doc = docx.Document(filepath)
            return '\n'.join(p.text for p in doc.paragraphs)
        except:
            return ''
    return ''

# ============================================================
# ä¸€è‡´æ€§æ£€æŸ¥
# ============================================================

def extract_snippets(content, cons_id):
    """ä»æ–‡æ¡£ä¸­æå–ä¸CONSè§„åˆ™ç›¸å…³çš„ç‰‡æ®µ"""
    rule = CONS_RULES[cons_id]
    snippets = []
    
    # æ£€æŸ¥å…³é”®è¯æ˜¯å¦å‡ºç°
    keywords_found = [kw for kw in rule['keywords'] if kw.lower() in content.lower()]
    if not keywords_found:
        return []
    
    # æå–ç›¸å…³æ®µè½
    if 'extract_pattern' in rule:
        matches = re.findall(rule['extract_pattern'], content, re.DOTALL | re.IGNORECASE)
        for m in matches:
            snippet = m.strip()[:500]  # æˆªå–å‰500å­—
            if snippet:
                snippets.append(snippet)
    
    return snippets

def check_terminology(content, filepath):
    """æ£€æŸ¥æœ¯è¯­å…¨ç§°ä¸€è‡´æ€§(CONS-10)"""
    issues = []
    for abbr, standard_full in STANDARD_TERMS.items():
        # æŸ¥æ‰¾æ–‡æ¡£ä¸­å¯¹è¯¥ç¼©ç•¥è¯­çš„å…¨ç§°å®šä¹‰
        patterns = [
            rf'{abbr}\s*[\(ï¼ˆ]\s*(.+?)\s*[\)ï¼‰]',       # CHS (Cybernetics of ...)
            rf'(.+?)\s*[\(ï¼ˆ]\s*{abbr}\s*[\)ï¼‰]',       # Cybernetics of ... (CHS)
            rf'{abbr}.*?(?:å³|is|refers to)\s*(.+?)[\nã€‚.]',  # CHSå³...
        ]
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                match = match.strip()
                if len(match) > 5 and match != standard_full:
                    # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥
                    if standard_full.lower() not in match.lower() and match.lower() not in standard_full.lower():
                        issues.append({
                            'abbr': abbr,
                            'found': match,
                            'standard': standard_full,
                            'file': str(filepath),
                        })
    return issues

def extract_numbers(content, keyword):
    """æå–å…³é”®è¯é™„è¿‘çš„æ•°å€¼ï¼ˆç”¨äºå·¥ç¨‹å‚æ•°ä¸€è‡´æ€§æ£€æŸ¥ï¼‰"""
    numbers = {}
    # æŸ¥æ‰¾å…³é”®è¯é™„è¿‘çš„æ•°å­—+å•ä½ç»„åˆ
    pattern = rf'{keyword}.*?(\d+[\d.,]*)\s*(km|å…¬é‡Œ|mÂ³/s|mÂ³|MW|ä¸‡kW|äº¿|åº§|ç±³)'
    matches = re.findall(pattern, content, re.IGNORECASE)
    for num, unit in matches:
        key = f"{num}{unit}"
        numbers[key] = True
    return numbers

def compare_snippets(snippets_by_doc, cons_id):
    """æ¯”è¾ƒä¸åŒæ–‡æ¡£ä¸­åŒä¸€CONSçš„è¡¨è¿°å·®å¼‚"""
    issues = []
    docs = list(snippets_by_doc.keys())
    
    if len(docs) < 2:
        return issues  # ä¸è¶³2ä¸ªæ–‡æ¡£ï¼Œæ— æ³•æ¯”è¾ƒ
    
    # æå–æ•°å€¼å‚æ•°è¿›è¡Œç²¾ç¡®æ¯”è¾ƒï¼ˆå·¥ç¨‹å‚æ•°ç±»CONSï¼‰
    if cons_id in ('CONS-06', 'CONS-07', 'CONS-08'):
        keyword_map = {
            'CONS-06': 'èƒ¶ä¸œ',
            'CONS-07': 'æ²™åª',
            'CONS-08': 'å—æ°´åŒ—è°ƒ',
        }
        kw = keyword_map[cons_id]
        all_numbers = {}
        for doc, snippets in snippets_by_doc.items():
            combined = ' '.join(snippets)
            nums = extract_numbers(combined, kw)
            all_numbers[doc] = nums
        
        # äº¤å‰æ¯”è¾ƒ
        ref_doc = docs[0]
        ref_nums = all_numbers[ref_doc]
        for doc in docs[1:]:
            doc_nums = all_numbers[doc]
            # å¦‚æœéƒ½æåˆ°äº†æ•°å€¼ä½†ä¸ä¸€è‡´
            if ref_nums and doc_nums:
                ref_set = set(ref_nums.keys())
                doc_set = set(doc_nums.keys())
                if ref_set != doc_set and ref_set and doc_set:
                    issues.append({
                        'cons_id': cons_id,
                        'type': 'æ•°å€¼å‚æ•°ä¸ä¸€è‡´',
                        'doc1': str(ref_doc),
                        'doc1_values': list(ref_set),
                        'doc2': str(doc),
                        'doc2_values': list(doc_set),
                    })
    
    return issues

# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def run_consistency_check(base_dir):
    """æ‰§è¡Œå®Œæ•´çš„è·¨æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥"""
    docs = find_documents(base_dir)
    
    if not docs:
        print(f"âš ï¸  åœ¨ {base_dir} ä¸­æœªæ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶")
        return True
    
    print(f"\n{'='*65}")
    print(f" ğŸ”— è·¨æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥æŠ¥å‘Š (CONS-01 ~ CONS-10)")
    print(f"{'='*65}")
    print(f"\nğŸ“‚ æ‰«æç›®å½•: {base_dir}")
    print(f"ğŸ“„ å‘ç°æ–‡æ¡£: {len(docs)}ä¸ª")
    for d in docs[:10]:
        print(f"   - {d.name}")
    if len(docs) > 10:
        print(f"   ... åŠå…¶ä»–{len(docs)-10}ä¸ªæ–‡ä»¶")
    
    all_issues = []
    cons_coverage = {}
    
    # é€æ¡CONSè§„åˆ™æ£€æŸ¥
    for cons_id, rule in CONS_RULES.items():
        snippets_by_doc = {}
        
        for doc_path in docs:
            content = read_document(doc_path)
            if not content:
                continue
            
            if rule.get('check_type') == 'terminology':
                # CONS-10: æœ¯è¯­ä¸€è‡´æ€§ç‰¹æ®Šå¤„ç†
                term_issues = check_terminology(content, doc_path)
                for ti in term_issues:
                    all_issues.append({
                        'cons_id': cons_id,
                        'level': 'WARNING',
                        'message': f'æœ¯è¯­{ti["abbr"]}å…¨ç§°ä¸ä¸€è‡´: å‘ç°"{ti["found"]}"ï¼Œæ ‡å‡†ä¸º"{ti["standard"]}"',
                        'file': ti['file'],
                    })
            else:
                snippets = extract_snippets(content, cons_id)
                if snippets:
                    snippets_by_doc[doc_path] = snippets
        
        # è®°å½•è¦†ç›–æƒ…å†µ
        if rule.get('check_type') != 'terminology':
            cons_coverage[cons_id] = len(snippets_by_doc)
            
            # æ¯”è¾ƒä¸åŒæ–‡æ¡£é—´çš„å·®å¼‚
            comparison_issues = compare_snippets(snippets_by_doc, cons_id)
            all_issues.extend(comparison_issues)
    
    # è¾“å‡ºè¦†ç›–ç»Ÿè®¡
    print(f"\nğŸ“Š CONSè§„åˆ™è¦†ç›–ç»Ÿè®¡:")
    for cons_id, rule in CONS_RULES.items():
        name = rule['name']
        count = cons_coverage.get(cons_id, '-')
        status = 'âœ…' if count and count != '-' and count >= 2 else 'âšª' if count == '-' or count == 0 else 'ğŸ“„'
        if count == '-':
            print(f"   {cons_id}: {status} {name} (æœ¯è¯­æ£€æŸ¥)")
        else:
            print(f"   {cons_id}: {status} {name} ({count}ä¸ªæ–‡æ¡£æ¶‰åŠ)")
    
    # è¾“å‡ºé—®é¢˜
    if all_issues:
        print(f"\nâš ï¸  å‘ç° {len(all_issues)} ä¸ªä¸€è‡´æ€§é—®é¢˜:")
        print(f"{'â€”'*55}")
        for i, issue in enumerate(all_issues, 1):
            cons_id = issue.get('cons_id', '?')
            if 'message' in issue:
                print(f"  {i}. [{cons_id}] {issue['message']}")
                if 'file' in issue:
                    print(f"     æ–‡ä»¶: {issue['file']}")
            elif 'type' in issue:
                print(f"  {i}. [{cons_id}] {issue['type']}")
                print(f"     æ–‡æ¡£1: {issue.get('doc1','')} â†’ {issue.get('doc1_values','')}")
                print(f"     æ–‡æ¡£2: {issue.get('doc2','')} â†’ {issue.get('doc2_values','')}")
    else:
        print(f"\nâœ… æœªå‘ç°ä¸€è‡´æ€§é—®é¢˜ã€‚")
    
    print(f"\n{'='*65}")
    if all_issues:
        print(f"âš ï¸  è¯·æ ¸å®å¹¶ç»Ÿä¸€ä¸ä¸€è‡´çš„è¡¨è¿°ã€‚ä»¥å…ˆå®Œæˆçš„æ–‡æ¡£ä¸ºå‡†ã€‚")
    else:
        print(f"âœ… è·¨æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡ã€‚")
    print()
    
    return len(all_issues) == 0

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='è·¨æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥ (CONS-01~CONS-10)')
    parser.add_argument('dir', help='æ–‡æ¡£ç›®å½•è·¯å¾„')
    args = parser.parse_args()
    
    passed = run_consistency_check(args.dir)
    sys.exit(0 if passed else 1)
