<!-- 变更日志
v2 2026-02-16: 根据四角色评审修订——补充基础题答案要点与术语一致性声明
v1 2026-02-16: 初稿
-->

# 第五章 强化学习与水系统控制

---

## 学习目标

完成本章后，你应能够：

1. 用马尔可夫决策过程（MDP）统一描述水系统调度问题；
2. 区分价值函数方法与策略梯度方法在水网控制中的适用边界；
3. 解释 Q-learning、DQN、PPO、SAC 的核心机制与工程差异；
4. 设计面向运行设计域（Operational Design Domain, ODD）与安全包络（Safety Envelope）的安全强化学习上线流程；
5. 评估强化学习策略在 SCADA+MAS Fusion Architecture 中的部署风险与回退机制。

> **章首衔接（承接ch04）**  
> 上一章通过 PINN 将机理方程嵌入学习过程，解决“预测要符合物理”的问题。本章转向“决策如何生成”：在已具备预测能力的前提下，如何让智能体在不确定环境中持续优化控制动作。

## 5.1 为什么需要强化学习

[物理直觉] 传统控制依赖清晰模型与固定目标，而真实水系统往往同时面对来水波动、设备约束、人工干预与多目标冲突。决策过程不是一次性优化，而是“观测—动作—反馈—再决策”的连续闭环。

[工程解释] 强化学习（Reinforcement Learning, RL）通过与环境交互学习策略，可在模型不完备、目标动态变化的场景中补充模型预测控制（Model Predictive Control, MPC）能力，特别适用于梯级协同与复杂工况切换。

## 5.2 MDP 建模框架

强化学习首先将问题抽象为 MDP：

\[
\mathcal{M}=\langle \mathcal{S},\mathcal{A},P,R,\gamma \rangle
\]

其中，\(\mathcal{S}\) 是状态集合，\(\mathcal{A}\) 是动作集合，\(P\) 为状态转移概率，\(R\) 为奖励函数，\(\gamma\in(0,1)\) 为折扣因子。

[工程意义] 对水系统而言，MDP 的关键不是公式本身，而是“状态、动作、奖励”三元定义是否贴合工程语义。定义不当会导致策略看似收敛但不可用。

### 5.2.1 状态设计

常见状态向量包括：
- 关键断面水位 \(h\)、流量 \(Q\)；
- 闸门开度、机组负荷、泵站启停状态；
- 未来短时来水预测、天气预警标识；
- ODD 工况标签（常态/汛期/检修/应急）。

### 5.2.2 动作设计

动作可为连续动作（如闸门目标开度）或离散动作（如机组组合档位）。

### 5.2.3 奖励设计

奖励函数通常是多目标加权：
\[
r_t = -w_1 J_{tracking} - w_2 J_{energy} - w_3 J_{risk} - w_4 J_{switch}
\]

其中 \(J_{tracking}\) 表征调度目标偏差，\(J_{energy}\) 表征能耗或弃水损失，\(J_{risk}\) 表征越界风险，\(J_{switch}\) 约束频繁动作切换。

[图5-1: 水系统强化学习MDP映射]
{描述: 左侧为状态输入（h,Q,设备状态,ODD标签），中间为策略网络，右侧为动作输出（闸门、机组、泵站），底部为奖励反馈与安全约束回路。}
{尺寸: 全页}
{颜色方案: 蓝色系}
{对应ARCH编号: ARCH-07}

## 5.3 经典算法与工程适配

### 5.3.1 Q-learning

[核心思想] 用 Q 值估计“在状态 \(s\) 下执行动作 \(a\) 的长期收益”，并通过贝尔曼更新迭代最优策略。

适用：小规模离散动作问题；
局限：状态维度高时易出现维数灾难。

### 5.3.2 DQN

DQN 用神经网络逼近 Q 函数，并通过经验回放与目标网络稳定训练。

适用：中等规模离散控制；
局限：对连续动作不友好，对奖励尺度敏感。

### 5.3.3 PPO

PPO 通过限制策略更新幅度提高训练稳定性，广泛用于连续控制。

适用：闸门开度连续调节、泵站变频控制；
局限：样本效率一般，在线迭代成本较高。

### 5.3.4 SAC

SAC 在目标中引入熵项，兼顾探索与利用，通常具备较好样本效率与鲁棒性。

适用：多工况切换、扰动较大的运行场景；
局限：超参数较多，部署前需充分离线验证。

[表5-1: 主要强化学习算法工程对比]
| 算法 | 动作空间 | 样本效率 | 稳定性 | 工程适配建议 |
|---|---|---|---|---|
| Q-learning | 离散 | 中 | 中 | 小规模规则优化 |
| DQN | 离散 | 中 | 中-高 | 离散设备组合控制 |
| PPO | 连续/离散 | 中-低 | 高 | 连续控制优先候选 |
| SAC | 连续 | 高 | 高 | 多扰动场景优先候选 |

## 5.4 安全强化学习：ODD 与 Safety Envelope 约束

[物理直觉] 强化学习天然倾向“试错”，但真实水系统容错空间有限。任何可能突破安全包络的探索都必须在仿真和在环测试中完成，不能直接在生产系统中冒险。

### 5.4.1 约束注入的三层路径

1. **奖励层**：对越界行为施加强惩罚；
2. **策略层**：动作投影到可行域，超限动作自动裁剪；
3. **系统层**：联锁与监督器拦截高风险指令。

### 5.4.2 最小风险状态与回退

当策略不确定性升高、或监测到 ODD 外工况时，系统应进入最小风险状态（Minimum Risk State）：
- 立即切换至保守控制律（如 MPC 基线策略）；
- 限制动作变化率，避免冲击；
- 提交人工确认后再恢复学习策略。

[图5-2: 安全强化学习上线门禁流程]
{描述: 离线训练→SIL验证→HIL验证→影子运行→分级上线，任一门禁失败均回退到基线控制。}
{尺寸: 全页}
{颜色方案: 蓝色系}
{对应ARCH编号: ARCH-06}

## 5.5 SCADA+MAS Fusion Architecture 中的部署范式

推荐采用“分层决策、分级授权”的部署策略：

- **L0-L1（现场层）**：保留硬联锁与快速保护；
- **L2（调度层）**：RL 产生建议动作，与 HDC 协调器融合；
- **L3（认知层）**：认知AI引擎负责策略解释、异常归因与人机协同审计。

[工程解释] 在该架构下，强化学习不是替代 SCADA，而是以“可审计策略插件”方式增强原有调度系统，降低组织阻力与上线风险。

## 5.6 例题：梯级水电“削峰+防洪”联合调度策略设计

【例5-1】某梯级系统在主汛期需要同时满足发电收益与防洪安全目标，要求设计一个可上线的强化学习策略原型。

**已知**：
- 上游来水预测时域为 24 小时；
- 关键约束包括库水位上限、泄量变化率与下游防洪流量红线；
- 系统已有 MPC 基线控制器。

**求解**：给出 RL 策略建模、训练、验证与上线方案。

**解题过程**：

步骤1：定义状态 \(\mathbf{x}_t=[h_t,Q_t,\hat{Q}_{t:t+24},\text{ODD}_t]\)。  
步骤2：定义动作为各站泄量增量与机组负荷分配。  
步骤3：构建奖励函数，将越界风险作为高权重惩罚项。  
步骤4：采用 SAC 在数字孪生环境离线训练，并记录策略不确定性。  
步骤5：通过 SIL/HIL 测试覆盖常态、暴雨、设备检修三类工况。  
步骤6：进入影子运行阶段，仅生成建议不直接下发。  
步骤7：满足门禁指标后分级放权，保留一键回退至 MPC。

**结果讨论**：

可上线策略的评价标准是“收益-安全-可治理”三者平衡，而非单一收益最大化。若策略解释性不足或越界拦截频繁，应延长影子运行周期。

### 强化学习章节上线检查清单

- [ ] 是否完成状态、动作、奖励的工程语义评审；
- [ ] 是否完成 ODD 外工况与极端事件测试；
- [ ] 是否配置动作可行域投影与联锁拦截；
- [ ] 是否建立策略不确定性监测与告警阈值；
- [ ] 是否验证一键回退链路可用且时延可接受。

## 5.7 本章小结

本章给出强化学习在水系统中的工程化方法：以 MDP 统一问题定义，以 Q-learning/DQN/PPO/SAC 形成算法工具箱，以 ODD 与 Safety Envelope 构建安全边界。关键结论是：强化学习的价值不在“替代一切”，而在“在可控风险内提升复杂场景决策质量”。下一章将进入大语言模型与认知AI，讨论如何把专家知识、文档语义与实时运行状态整合为可交互的调度认知能力。

## 习题

### 基础题

1. 为什么水系统强化学习必须先完成 MDP 的工程语义建模？  
2. DQN 与 PPO 在动作空间假设上有何差异？  
3. 什么情况下应触发最小风险状态（Minimum Risk State）？

### 基础题参考答案要点

1. 因为状态、动作、奖励的定义直接决定策略学习方向；若工程语义缺失，策略会优化错误目标。  
2. DQN主要面向离散动作并通过Q值选最大动作；PPO可处理连续动作并通过策略梯度直接更新策略。  
3. 当策略不确定性超阈值、检测到ODD外工况、或动作可能触发Safety Envelope越界时应立即触发。

> **术语一致性声明**：本章统一采用“运行设计域（Operational Design Domain, ODD）”“安全包络（Safety Envelope）”“分层分布式控制（Hierarchical Distributed Control, HDC）”“多智能体系统（Multi-Agent System, MAS）”“SCADA+MAS Fusion Architecture”“模型预测控制（Model Predictive Control, MPC）”“水网自主等级（Water Network Autonomy Levels, WNAL）”等规范术语，并与前序章节保持一致。

### 应用题

4. 设计一个泵站群协同控制的奖励函数，并说明每一项工程含义。  
5. 给出“离线训练—影子运行—分级放权”的门禁指标建议。

### 思考题

6. 当 RL 策略与值班工程师经验冲突时，系统应如何实现人机共融决策？请提出可执行流程。

## 拓展阅读

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*.  
2. Mnih et al. (2015). Human-level control through deep reinforcement learning.  
3. Schulman et al. (2017). Proximal Policy Optimization Algorithms.  
4. Haarnoja et al. (2018). Soft Actor-Critic.  
5. Lei et al. (2025c). 在环测试系统。
