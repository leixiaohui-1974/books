# 第7章 认知增强与在线学习：可学习但不失控

## 学习目标

- 回顾第6章在环验证框架，理解在线学习策略为何必须受治理约束。
- 掌握认知增强模块在SCADA+MAS Fusion Architecture中的职责与边界。
- 能够建立“学习模块准入门槛—在线监测—异常回退”闭环机制。
- 理解学习策略异常漂移的检测指标、告警分级与处置流程。
- 为第8章工程化部署与运维协同提供可实施的学习治理基座。

---

## 7.1 章节衔接：从“验证可通过”到“演进可控制”

第6章解决了策略上线前验证问题，但认知增强系统还面临“上线后持续演化”挑战：数据分布变化、工况迁移、设备老化都会使既有模型逐步失配。在线学习可以提升适应性，但若缺少约束，会把系统从“可优化”推向“不可控”。因此，本章核心命题是：**让系统可学习，但永远不突破ODD与Safety Envelope。**

---

## 7.2 认知增强模块的角色定位

认知增强（Cognitive AI Engine）不替代控制闭环，而是增强三类能力：

1. **解释增强**：把复杂工况转译为可执行建议与风险说明；
2. **预测增强**：对来水、负荷、异常趋势做短中期预测；
3. **协商增强**：为多智能体冲突仲裁提供上下文证据。

边界原则：
- 认知模块可“建议”，不可绕过监督智能体直接“执行”；
- 任何学习输出必须经过硬约束校核与审计记录。

---

## 7.3 学习模块准入门槛

[表7-1: 学习模块准入门槛]

| 维度 | 准入条件（建议） | 不满足处置 |
|---|---|---|
| 数据质量 | 缺失率 < 2%，时间对齐误差 < 1采样周期 | 阻止上线，回退基线模型 |
| 验证表现 | SIM/SIL/HIL全通过，NVR=0 | 继续离线训练 |
| 覆盖度 | $C_{all} \ge 0.85$ 且异常路径覆盖达标 | 补测后复评 |
| 稳定性 | 连续窗口内输出波动率低于阈值 | 降低学习率或冻结参数 |
| 可解释性 | 关键建议可追溯至特征与规则 | 标记为仅观察模式 |

该门槛将“能训练”与“能上线”明确区分，避免实验模型直接进入生产链路。

---

## 7.4 在线学习治理闭环

### 7.4.1 三阶段闭环

- **阶段A（观察）**：学习模型只产生影子输出，与基线策略对比；
- **阶段B（受限影响）**：学习输出仅可调节软约束权重，不可更改硬约束；
- **阶段C（受控接管）**：在明确ODD子域内允许学习策略主导，异常即刻回退。

### 7.4.2 形式化约束

记学习策略为 $\pi_L$，基线策略为 $\pi_B$，可执行策略为：

$$
\pi_{exec}(k)=\lambda(k)\pi_L(k)+(1-\lambda(k))\pi_B(k),\quad \lambda(k)\in[0,\lambda_{max}]
$$

并满足：

$$
\mathbf{u}(k)=\Pi_{\Omega_{safe}}\big(\pi_{exec}(k)\big)
$$

其中 $\Pi_{\Omega_{safe}}$ 为安全可行域投影。即便学习策略失真，最终输出仍被硬约束投影保护。

[图7-1: 在线学习治理闭环]
{描述: 展示观察、受限影响、受控接管三阶段，以及监测、告警、回退的闭环箭头；标注与监督智能体的校核接口。}
{尺寸: 全页}
{颜色方案: 蓝色系}

---

## 7.5 异常漂移检测与告警流程

### 7.5.1 漂移检测指标

推荐同时监测：
- **数据漂移**：输入分布距离（如PSI/KL）；
- **性能漂移**：预测误差、约束裕度恶化；
- **行为漂移**：动作频谱突变、策略切换频率异常。

### 7.5.2 告警分级

- **L1 提醒**：轻微漂移，继续观察；
- **L2 警告**：中度漂移，冻结学习率并启动复评；
- **L3 严重**：高风险漂移，立即回退基线策略并触发人工接管。

[图7-2: 学习策略异常漂移告警流程]
{描述: 从漂移检测到分级告警、自动限权、模型冻结、基线回退、人工复核的流程图。}
{尺寸: 半页}
{颜色方案: 灰度}

---

## 7.6 例题：学习策略漂移下的自动限权与回退

【例7-1】某在线学习调度器在连续30分钟内出现误差上升：预测误差由8%升至19%，动作切换频率增加2.5倍。监督智能体检测到硬约束裕度快速缩小，但尚未越界。

**已知**：
- 当前融合权重上限 $\lambda_{max}=0.6$；
- 漂移等级判定为L2；
- 基线策略可即时接管。

**求解**：给出自动处置流程，并说明何时升级到L3回退。

**解题过程**：
1. 触发L2警告，立即将 $\lambda_{max}$ 下调至0.2，限制学习策略影响；
2. 冻结在线参数更新，进入“观察窗口”；
3. 若观察窗口内误差与切换频率恢复，则逐步恢复权重；
4. 若误差继续上升或触发硬约束风险阈值，则升级L3；
5. L3执行：$\lambda\leftarrow0$，完全切回基线策略并人工复核。

**结果讨论**：
该流程体现“先限权、后回退”的分级治理思想，可在不立即中断服务的前提下控制风险扩散，并保留快速切回安全基线的能力。

---

## 7.7 本章小结

本章围绕“可学习但不失控”构建了认知增强与在线学习治理框架：首先明确认知模块只能增强解释、预测与协商，不可绕过监督链路；其次提出学习模块准入门槛，确保模型上线前在数据质量、覆盖度、稳定性和可解释性上达标；再次通过融合权重受限与安全投影机制，实现在线学习的可控接入；最后建立异常漂移分级告警与回退流程，保证策略演进始终处于审计可追踪和风险可收敛状态。下一章将基于该治理框架，讨论工程化运维协同与长期演进机制。

---

## 习题

### 一、基础题

1. 为什么认知增强模块不能绕过监督智能体直接下发执行指令？
2. 解释学习模块准入门槛中“覆盖度达标”的必要性。
3. 比较L2与L3漂移告警的处置差异。

### 二、应用题

4. 设某时刻 $\pi_L=0.8,\ \pi_B=0.3,\ \lambda=0.25$（标量示意），计算融合输出并说明限权对风险控制的作用。
5. 设计一个在线学习策略的周度复评清单（不少于5项），用于决定是否继续放宽 $\lambda_{max}$。

### 三、思考题

6. 在强不确定性工况下，“更快学习”与“更稳治理”之间如何平衡？请给出你认可的工程决策原则。

---

## 拓展阅读

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*.
2. Gama, J., Žliobaitė, I., Bifet, A., et al. (2014). A survey on concept drift adaptation. *ACM Computing Surveys*.
3. Lei, X. (2025b). 自主智能水网架构. DOI: 10.13476/j.cnki.nsbdqk.2025.0079.
4. Lei, X. (2025c). 在环测试系统. DOI: 10.13476/j.cnki.nsbdqk.2025.0080.
5. Koopman, P. (2022). *How Safe Is Safe Enough?*.

---

**下一章预告**：第8章将讨论工程化运维协同，包括组织流程、权限治理、版本管理与长期演进路线图。
